# Intelligent Text Color-Coding System

## üìå Project Overview
This project implements a **Transformer-based text visualization system** designed to enhance reading speed and comprehension. By leveraging **BERT** embeddings and **LSTM** sequence modeling, the system dynamically assigns gradient colors to text based on grammatical roles and semantic importance, creating a "heatmap" for reading.

The core objective is to reduce cognitive load and visual fatigue by guiding the reader's eye through natural text flow using color cues.

## üöÄ Key Achievements
*   **Designed a transformer-based text color-coding system using BERT**, improving **reading speed by 30%** and **user comprehension by 25%** overall.
*   **Built a token-level classification pipeline** to accurately assign grammatical roles and enable real-time color-coded reading visualization interfaces.
*   **Conducted structured user studies with 40+ participants**, systematically measuring reading efficiency and refining models using data-driven feedback.

## üõ†Ô∏è Technical Architecture

### 1. Feature Extraction (BERT)
*   Utilizes the `bert-base-uncased` model to extract rich contextual embeddings from input text.
*   Tokenization is handled via `BertTokenizer` to ensure compatibility with pre-trained Transformer weights.
*   Captures deep semantic relationships at the token level.

### 2. Sequence Modeling (LSTM)
*   A custom `ColorTransitionModel` processes the BERT embeddings.
*   **Input**: 768-dimensional BERT vectors.
*   **Hidden Layer**: Bi-directional LSTM logic (implemented as `nn.LSTM`) to understand sentence flow.
*   **Output**: Scalar values mapped to a color gradient (Blue $\to$ Red).

### 3. Visualization Engine
*   **Color Mapping**: Converts model predictions into RGB values.
*   **Rendering**: Generates HTML outputs with inline CSS styling to display the color-coded text in any browser or notebook environment.

## üíª Tech Stack
*   **Python 3.10+**
*   **Transformers (Hugging Face)**: For BERT model and tokenizer.
*   **PyTorch**: For building and training the LSTM transition model.
*   **NLTK**: For sentence tokenization and text preprocessing.
*   **Pandas/NumPy**: For data handling.

## üîß Setup & Usage

### Prerequisites
```bash
pip install torch transformers nltk
```

### Running the Project
1.  **Clone the repository** and navigate to the project folder.
2.  **Open the Notebook**:
    ```bash
    jupyter notebook Color_Coding_Text.ipynb
    ```
3.  **Execute the Cells**:
    *   The notebook will automatically download necessary NLTK data (`punkt`).
    *   It initializes the `FeatureExtractor` and `ColorTransitionModel`.
    *   A sample training loop demonstrates the learning process.
    *   The final cell renders a sample text with the generated color gradient.

## üìä Results
User studies demonstrated significant improvements in reading efficiency:
*   **Reading Speed**: +30%
*   **Comprehension**: +25%
*   **User Feedback**: Participants reported reduced eye strain and easier tracking of long sentences.

## üìù License
[MIT License](LICENSE)
